{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2st7DsxmhEdhqIVa/ntZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1906miral/PFE/blob/main/weightsfile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TKFX8-fZeH5",
        "outputId": "9e129e06-ce97-4ebb-bfd7-2ebdd0a10779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 77.34%\n",
            "Saved model to disk\n",
            "Loaded model from disk\n",
            "accuracy: 77.34%\n"
          ]
        }
      ],
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy\n",
        "import os\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"diabetes.csv\", delimiter=\",\",skiprows=1)\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# later...\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyYAML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7J8mHUMahNf",
        "outputId": "17ec081c-f928-4d5d-a768-f98d73b61c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP for Pima Indians Dataset serialize to YAML and HDF5\n",
        "from tensorflow.keras.models import Sequential, model_from_yaml\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy\n",
        "import os\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"diabetes.csv\", delimiter=\",\",skiprows=1)\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# later...\n",
        "\n",
        "# load YAML and create model\n",
        "yaml_file = open('model.yaml', 'r')\n",
        "loaded_model_yaml = yaml_file.read()\n",
        "yaml_file.close()\n",
        "loaded_model = model_from_yaml(loaded_model_yaml)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "metadata": {
        "id": "lR9-xEk6aqHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "# Open the HDF5 file\n",
        "with h5py.File('model.h5', 'r') as f:\n",
        "    # Print the keys at the root level of the file\n",
        "    print(list(f.keys()))\n",
        "\n",
        "    # Access a specific group or dataset\n",
        "    # In your case, it could be one of the keys printed above, such as 'dense', 'dense_1', etc.\n",
        "    group_or_dataset = f['dense']\n",
        "\n",
        "    # Access the weights dataset of the layer\n",
        "    weights_dataset = group_or_dataset['dense']['kernel:0']\n",
        "\n",
        "    # Get the actual values of the weights\n",
        "    weights_values = weights_dataset[()]\n",
        "\n",
        "    # Print the weights\n",
        "    print(weights_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOIY2je2cm_q",
        "outputId": "fb191a65-bd16-4d56-d769-1a08e60df6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dense', 'dense_1', 'dense_2', 'top_level_model_weights']\n",
            "[[-0.08412638  0.26798606 -0.169175    0.2392254   0.3549289  -0.37569845\n",
            "  -0.42245486 -0.24219687 -0.16547212  0.04396991  0.8731155  -0.11247029]\n",
            " [-0.32441005 -0.47212577  0.34215823  0.02278689 -0.08264944 -0.10908684\n",
            "   0.46206376  0.17672534  0.18619376  0.20884013  0.25806594 -0.60321194]\n",
            " [-0.28440672  0.02111932  0.38090628  0.05009577 -0.17924339 -0.2664191\n",
            "   0.33164293  0.05871939 -0.2565466  -0.61293787 -0.16904718  0.25336447]\n",
            " [-0.227889   -0.07462751 -0.21090911 -0.4422397  -0.12878618 -0.3372593\n",
            "  -0.15101871 -0.17185971  0.26292744 -0.27282935  0.44622567  0.52875036]\n",
            " [ 0.11847184  0.02972206  0.33427855 -0.418566   -0.44185174 -0.30716887\n",
            "  -0.06029643  0.12823713 -0.05391832 -0.54524076  0.14520226  0.06863838]\n",
            " [ 0.23692165 -0.01793516 -0.3443014   0.36863565  0.21677822  0.22840583\n",
            "  -0.4598091  -0.2178675   0.4096717  -0.00347554  0.16337857 -0.21845365]\n",
            " [-0.8361968   0.3600982  -0.8553785  -0.03604641 -0.21912172  0.2680794\n",
            "  -1.0676074   0.6117458  -0.8405871   0.3718915   0.96818906  0.4421978 ]\n",
            " [ 0.48064956 -0.03617705 -0.21644945  0.3715103  -0.28267163 -0.08370602\n",
            "   0.48519433 -0.14978716 -0.17726782  0.24529386  0.00984041  0.01385794]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TT6pZxWxc4rQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}